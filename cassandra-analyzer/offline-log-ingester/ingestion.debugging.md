# Ingestion - Debugging
Listed below are some common errors you might run into while running offling-log-ingester, as well as potential solutions. 

### Table of Contents
- [General Debugging Techniques](#General-Debugging-Techniques)
- [Elasticsearch related issues](#Elasticsearch-related-issues)
- [Filebeat related issues](#Filebeat-related-issues)
- [Kibana related issues](#Kibana-related-issues)
- [Other Issues](#Other-Issues)

# General Debugging Techniques
Here are some tricks and tips you might try in general, that might be able to help you find out what the issue is for a number of errors you run into.

## Debugging your log tarball
Your tarball needs to be in the same format that is generated either using [offline-log-collector](../offline-log-collector/README.md) or from getting a diagnostic tarball from DSE opscenter. 

If you have a tarball in a different format, or if you are having trouble using our tool and need to double check, [click here to find out what format our tool expects the log tarball to be in](./ingestion.tarball-format.md).

## Debugging the offline-log-ingester `filebeat.yaml` generator
Sometimes the filebeat.yaml file that our Python script generates comes out incorrectly, or in a way that you did not expect. This could make it so it doesn't see your log files at all, sees only some log files, does not ingest log files to elasticsearch or kibana correctly, and so on. Here are some things you can do to debug the generated `filebeat.yaml` file:

### Find and read through the file
The generated filebeat.yml file can be found at:
```
cassandra.vision/cassandra-analyzer/offline-log-ingester/logs-for-client/{client_name}/incident-{incident_id}/tmp/filebeat.yaml
```
- `{client_name}` is the name you specified in the first arg when you ran the python script
- `{incident_id}` is generated by our script, and is a unique string based on a timestamp from when the tarball was last modified.

For example, if `client_name` is "test_client" and `incident-id` is "1620704941.7606971":

```
cassandra.vision/cassandra-analyzer/offline-log-ingester/logs-for-client/test_client/incident-1620704941.7606971/tmp/filebeat.yaml
```

### Try editing the filebeat.yml manually and running again 
Sometimes when you are debugging, you want to just debug the generated filebeat.yml file. If so, you can just edit the file directly and run filebeat again with the edited filebeat.yml file. 

Note however that if you run offline-log-ingester again, it will generate a new write filebeat.yml file and any changes you make will be overwritten by the newly generated file. You can move your filebeat.yml to somewhere else to get around this. 

See [instructions here](./README.md#want-to-run-filebeat-again-with-the-same-config) for running the generated yml again.

## Debugging your filebeat installation
### Try `sudo filebeat setup`
Sometimes filebeat will process logs correctly (which you will be able to see in the filebeat log output, since it will show a log (level DEBUG) for event "Publish event" that shows all the fields. However, it won't get into kibana correctly. Sometimes all it takes is running `sudo filebeat setup` so that filebeat configures for the current elasticsearch setup

Note that by default, `sudo filebeat setup` will use your default filebeat.yml file, which is found at `/etc/filebeat/filebeat.yml`. Make sure those settings are correct, since even running filebeat with a different filebeat.yml will not override some of these configs (especially configs under the `setup` property, e.g., `setup.template.settings`). Those settings only get set when running `filebeat setup`. 

If you want to setup filebeat using a different filebeat.yml file, you can use the `--c` flag, e.g.,:

```
sudo filebeat setup --c cassandra.vision/cassandra-analyzer/offline-log-ingester/logs-for-client/{client_name}/incident-{incident_id}/tmp/filebeat.yaml
```

## Helpful Elasticsearch queries to debug
### Check if Filebeat created an index in Elasticsearch
This is helpful for making sure filebeat is correctly ingesting into elasticsearch.
Assuming elasticsearch is running at `localhost`:
```
curl localhost:9200/_cat/indices/filebeat-*
```

If filebeat created an index, you should see something like this:

```
=> curl localhost:9200/_cat/indices/filebeat-*
yellow open filebeat-7.12.1-2021.05.11-000001 pnU1SQrlSgu0diglKmOtYQ 1 1 15124 0 4.1mb 4.1mb
```

# Elasticsearch Related Issues

## ERROR: `ConnectionError(('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))`
Your elasticsearch or kibana hosts might need to be set if you get an error that looks like the following after running the script:
```
elasticsearch.exceptions.ConnectionError: ConnectionError(('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))) caused by: ProtocolError(('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))
```

You can find instructions for doing that above under [Specifying Kibana endpoint](#Specifying Kibana endpoint).

ES host can be set using --es-hosts flag as well.

## ERROR: `Elasticsearch.exceptions.ConnectionError: NewConnectionError`
E.g., 
```
Elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x7fab672f0780>: Failed to establish a new connection: [Errno 111] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7fab672f0780>: Failed to establish a new connection: [Errno 111] Connection refused)
```

Diagnosing this one:
- There's probably also something like this in the stacktrace:
```
...
File "ingest_tarball.py", line 275, in clear_filebeat_indices_and_registry
self.es.indices.delete(index='filebeat-*')
...
```

- Also you are probably using `--clean-out-filebeat-first` flag, since this is the only thing we're using the python client for right now.

Solution: 
ES Python client isn't connecting to elasticsearch. You probably need to set the --es-hosts flag to something else.

# Filebeat related Issues
## ERROR: data path already locked by another beat. 
E.g., if when running offline-log-ingester you see something like this:
```
2022-06-09T15:27:17.387+0700    ERROR   instance/beat.go:971    Exiting: data path already locked by another beat. Please make sure that multiple beats are
not sharing the same data path (path.data).
Exiting: data path already locked by another beat. Please make sure that multiple beats are not sharing the same data path (path.data).
```

It probably means filebeat is already running, and needs to be stopped. [This StackOverflow post](https://stackoverflow.com/questions/65561985/filebeat-data-path-already-locked-by-another-beat-please-make-sure-that-multi) provides more information. 

It can be stopped by running:
```
sudo systemctl stop filebeat
```

If not, you might need to see if you have a `filebeat.lock` file that accidentally never got removed (e.g., at `/var/lib/filebeat/filebeat.lock`). If so, try removing that and try again (NOTE be careful doing this, if you don't know what you're doing! Make sure filebeat isn't running already first!).


## ERROR: sudo: filebeat: command not found
E.g., if when running offline-log-ingester you see something like this:

```
=== Running Filebeat ===
Running filebeat command: sudo filebeat -e -d "*" --c cassandra.vision/cassandra-analyzer/offline-log-ingester/logs-for-client/test_client/incident-1620704941.7606971/tmp/filebeat.yaml
sudo: filebeat: command not found
```

### Solution
You need to have filebeat installed on your system and accessible from your path. [Follow instructions here](./README.md#step-10-prerequisites).

# Kibana related issues
NOTE besides issues below, also see [Kibana ingestion guide](../kibana-dashboard/README.md#debugging) especially for help debugging dashboard import issues.

## No Data is Showing Up in Kibana
If there's no data visible in Kibana but you see your log data in Elasticsearch (which you can determine several ways, including using [Elasticsearch's REST API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html)) then you probably have one of two problems:

1) Kibana is not connected to Elasticsearch correctly
    - If you used our ansible playbook, it should be setup correctly already. 
    - If not, you can [see the settings which are available for Kibana here](https://www.elastic.co/guide/en/kibana/current/settings.html). For example, you might need to change `elasticsearch.hosts` in your `kibana.yml`.
2) Kibana is connected, but you need to setup your index pattern in Kibana first. 
    - You can determine if this is the issue by creating an index pattern, either manually or by importing our dashboard. [Click here for instructions](./README.md#Step-4-View-the-Logs-in-Kibana).


Further reading: 
- https://www.digitalocean.com/community/tutorials/how-to-troubleshoot-common-elk-stack-issues


# Other Issues
## ERROR: `FileNotFoundError: <...>/nodes/nodes`
E.g., 
```
FileNotFoundError: [Errno 2] No such file or directory: '<path-to-project>/cassandra.vision/cassandra-analyzer/offline-log-ingester/logs-for-client/<client-name>/incident-<incident-id>/tmp/nodes/nodes'
```
This means that the ingested tarball looked something like this: 
```
<tarball-filename>.tar.gz OR <tarball-filename>.zip
        nodes/
            <ip-1>/
            ...
```
The tarball is missing the `<archived-dir>/`, so our script thinks that `nodes` is the name of the `<archived-dir>/`. Consequently, it's looking for `nodes/nodes/` but can't find it. 

### Potential Solutions
- Reformat your tarball (i.e., put the `nodes` dir inside a parent directory) and zip it back up, and run script again. 
    - The tarball needs to be in the format [described here](./ingestion.tarball-format.md). 
    - In this particular case, you can reformat your tarball doing something like this:
      ```
      # navigate to where your extracted files are
      cd offline-log-ingester/logs-for-client/<client-name>/incident-<incident-id>/tmp

      # make a new parent dir
      mkdir new-parent-dir

      # put the `nodes` dir inside a parent directory
      mv ./nodes ./new-parent-dir/

      # zip it back up to where the script expects the tarballs to go
      tar -czvf ../log-tarballs-to-ingest/new-tarball.tar.gz ./new-parent-dir

      # run offline-log-ingester again
      python3 ingest_tarball.py new-tarball.tar.gz my_client
      ```

- Do dangerous stuff if you are feeling confident that you know what you're doing. (DANGER)
    - E.g., going into the `./logs-for-client/` dir where the unzipped logs get put into, and rearrange things there, and temporarily commenting out steps in the `run` method of the `ingest_tarball.py` script, particularly `self.extract_tarball()`, then running `ingest_tarball.py` again. 
    - Obviously less than ideal.


## ERROR: `failed to open store 'filebeat': open /var/lib/filebeat/registry/filebeat/meta.json: no such file or directory`
### Diagnosis
This seems to be because newer versions of ES/filebeat (e.g., 7.9.x) have different behavior, and to clear out the registry you actually need to remove the whole `/var/lib/filebeat/registry`, not just the subdirectory `/var/lib/filebeat/registry/filebeat`, which worked before. 

If you don't delete the `/var/lib/filebeat/registry` directory, filebeat doesn't seem to know that it needs to regenerate that directory for you, and throws that error.

### Solution 
`rm -rf /var/lib/filebeat/registry`

For reference, see [here](https://discuss.elastic.co/t/cant-start-filebeat/181050/7).

