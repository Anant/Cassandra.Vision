# Ingest Tarball of logs into Kibana
## What is "offline log analysis"?
Sometimes it is preferable to do "online log analysis", which is where you collect logs on a live cluster and ingest into Elasticsearch/Kibana (or some other dashboard). However, there are situations where this is not possible or preferable and you want to grab some logs, put them in a tarball, and ingest into your dashboard, often running in a separate host. We call this "offline log analysis". 

After you collect your logs and grab other diagnostic data, you need to ingest it into your Dashboard. This is what this tool is for. Specifically, we ingest into Elasticsearch and Kibana using Filebeat.

## Setup
- Requires python3 and pip3
- `pip3 install -r requirements.txt`
- Place a log tarball in `./log-tarballs-to-ingest/` (currently not automating, you have to do this)
    * Make sure that this is the tarball either generated by our `collect_logs.py` script or by Opscenter. If this is a .zip, the script will unarchive the archive still but will likely fail unless the directory layout is exactly what DSE opsecenter returns. 
    * Having a directory like this gives us modularity and makes it easy to change. We can manually do this (`mv my.tgz ./log-tarballs-to-ingest/`) for now, and easily later add a script that does this for us, or even expose a web GUI for uploading it in. Then whatever we do, we place these tars in this directory

- Make sure ES and Kibana are running already (we will start filebeat later)
- Run a script, passing in certain metadata about the tarball. E.g., 
    ```
    python3 ingest_tarball.py my-client-logs-tarball.tar.gz my_client
    ```

    Or, if you want to clear out your filebeat indices first and filebeat registry:
    ```
    python3 ingest_tarball.py my-client-logs-tarball.tar.gz my_client --clean-out-filebeat-first
    ```
### Specifying Kibana endpoint
By default the script is pointing towards a kibana instance running on localhost. To specify a different kibana host, use the `--custom-config arg`:
    To pass in arbitrary config for the filebeat.yml file, send in a key (can be nested) and a value, e.g., 
    ```
    --custom-config setup.kibana.host 123.456.345.123:5601
    ```

### Other options
    1) You can also use `debug_mode` which doesn't write any logs to ES, only outputs to console by using the `--debug-mode` flag:
    ```
    python3 ingest_tarball.py my-client-logs-tarball.tar.gz my_client --debug-mode
    ```

    2) To pass in arbitrary config for the filebeat.yml file, use the `--custom-config` flag  send in a key (can be nested) and a value, e.g., 
    ```
    --custom-config setup.kibana.host 123.456.345.123:5601
    ```

    3) To cleanup all generated files if the script run successfully, pass in:
    ```
    --cleanup-on-finish
    ```

    4) ignore zeros in tarball (for when using a combined tarball; see [here](https://www.gnu.org/software/tar/manual/html_node/Ignore-Zeros.html) for what we do)
    ```
    --ignore-zeros 
    ```
    NOTE currently only works with gzipped tarballs (ie file extension tar.gz)

## What does the script do?
  - unzip the tarball
  - Put the logs in the folder we want it in
  - Generate a filebeat.yml for this (will be v0.2; v0.1 just write this ourselves)
  - start filebeat for one-off batch job that ingests these files into ELK 
      * Perhaps later we will just have filebeat running continually on our server, watching  whatever gets placed in

## Want to add some logs and run script again with the same config?
1) Add log files to the directory where similar logs are located: 
  `{self.base_filepath_for_logs}/<hostname>/<type>`.

  e.g., `{self.base_filepath_for_logs}/{example_hostname}/spark/worker/worker.log` 

2) Run filebeat again:

Replace the client_name and incident_id below and run it again
```
sudo filebeat -e -d "*" --c cassandra.toolkit/log-analysis/automated-tarball-ingestion/logs-for-client/{client_name}/incident-{incident_id}/tmp/filebeat.yaml
```

- filebeat.yaml will be at: cassandra.toolkit/log-analysis/automated-tarball-ingestion/logs-for-client/{client_name}/incident-{incident_id}/tmp/filebeat.yml
- Alternatively, if filebeat is still running (and is using the filebeat.yaml created by this script), you can just add the separate log files and it will find and ingest them. 

## Debugging
### Debugging the filebeat generator
  - If you want to make some manual changes to the filebeat.yml that was generated and try again, you can run:
      ```
      sudo filebeat -e -d "*" --c $PWD/logs-for-client/my_client/incident-159687225/tmp/filebeat.yaml
      ```
      (substituting in the real path for the filebeat.yaml that was generated)


### Debugging ES
#### Try sudo filebeat setup
Sometimes filebeat will process logs correctly (which you will be able to see in the filebeat log output, since it will show a log (level DEBUG) for event "Publish event"that shows all the fields. However, it won't get into kibana correctly. Sometimes all it takes is running `sudo filebeat setup` so that filebeat configures for the current elasticsearch setup

Note that by default, `sudo filebeat setup` will use your default filebeat.yml file, which is found at `/etc/filebeat/filebeat.yml`. Make sure those settings are correct, since even running filebeat with a different filebeat.yml will not override some of these configs (especially configs under the `setup` property, e.g., `setup.template.settings`). Those settings only get set when running `filebeat setup`. 

If you want to setup filebeat using a different filebeat.yml file, you can use the `--c` flag, e.g.,:

```
sudo filebeat setup --c cassandra.toolkit/log-analysis/automated-tarball-ingestion/logs-for-client/{client_name}/incident-{incident_id}/tmp/filebeat.yaml
```

#### Error: ConnectionError(('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))
Your elasticsearch or kibana hosts might need to be set if you get an error that looks like the following after running the script:
```
elasticsearch.exceptions.ConnectionError: ConnectionError(('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))) caused by: ProtocolError(('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))
```

You can find instructions for doing that above under [Specifying Kibana endpoint](#Specifying Kibana endpoint).

## Testing
### Current way to test: 
Run collect_logs test first. Then run ingest_tarball.py on that test tarball:

```
cd
pip3 install -r requirements.txt
pip3 install -r test/requirements.txt
cdtest
python3 collect_logs_test.py
cd..
python3 ingest_tarball.py  test_client.tar.gz test_client 
# Or, if you are safe to wipe out filebeat registry and filebeat indices before running the test:
# python3 ingest_tarball.py  test_client.tar.gz test_client --clean-out-filebeat-first

```

### OLD WAY: use the test class
NOTE Currently out of date. 
Would do:
```
  pip3 install -r requirements.txt
  pip3 install -r test/requirements.txt
  cd test
  python3 ingest_tarball_test.py
```

Will need to update `ingest_tarball_test.py` for this to work though.

