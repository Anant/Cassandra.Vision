# Ingest Tarball of logs into Kibana
## What is "offline log analysis"?
Sometimes it is preferable to do "online log analysis", which is where you collect logs on a live cluster and ingest into Elasticsearch/Kibana (or some other dashboard). However, there are situations where this is not possible or preferable and you want to grab some logs, put them in a tarball, and ingest into your dashboard, often running in a separate host. We call this "offline log analysis". 

After you collect your logs and grab other diagnostic data, you need to ingest it into your Dashboard. This is what this tool is for. Specifically, we ingest into Elasticsearch and Kibana using Filebeat.

Note that we have a starter Kibana Dashboard that you can import into kibana as well (see separate directory "kibana-dashboard"). [Find it here on Github](https://github.com/Anant/cassandra.vision/tree/master/cassandra-analyzer/kibana-dashboard).

## Setup
- Requires python3 and pip3
- Currently also requires `filebeat` to be callable from the commandline.
- `pip3 install -r requirements.txt`
- Place a log tarball in `./log-tarballs-to-ingest/` (currently not automating, you have to do this)
    * Make sure that this is the tarball either generated by our `collect_logs.py` script or by Opscenter. If this is a .zip, the script will unarchive the archive still but will likely fail unless the directory layout is exactly what DSE opsecenter returns. 
    * Having a directory like this gives us modularity and makes it easy to change. We can manually do this (`mv my.tgz ./log-tarballs-to-ingest/`) for now, and easily later add a script that does this for us, or even expose a web GUI for uploading it in. Then whatever we do, we place these tars in this directory

- Make sure ES and Kibana are running already (we will start filebeat later)
- Run a script, passing in certain metadata about the tarball. E.g., 
    ```
    python3 ingest_tarball.py my-client-logs-tarball.tar.gz my_client
    ```

    Or, if you want to clear out your filebeat indices first and filebeat registry:
    ```
    python3 ingest_tarball.py my-client-logs-tarball.tar.gz my_client --clean-out-filebeat-first
    ```
### Specifying Elasticsearch host
By default the script is pointing towards a elasticsearch instance running on localhost. To specify a different host, use the `--es-host` arg:
    ```
    --es-hosts 123.456.345.123:9200
    ```

- this will be used to set `output.elasticsearch.hosts` in the filebeat yaml
- unlike using --custom-config, this will set the es host for the es python client the script uses. Right now the client only gets used if you also use the `--clean-out-filebeat-first` flag, but that might change in the future. 
- if you also set a `--custom-config` flag for `output.elasticsearch.hosts`, that will override what you set for es-host in the yaml, but will not be used for the es python client.

### Specifying Kibana endpoint
By default the script is pointing towards a kibana instance running on localhost. To specify a different kibana host, use the `--custom-config` arg:
    To pass in arbitrary config for the filebeat.yml file, send in a key (can be nested) and a value, e.g., 
    ```
    --custom-config setup.kibana.host 123.456.345.123:5601
    ```

### Other options
    1) You can also use `debug_mode` which doesn't write any logs to ES, only outputs to console by using the `--debug-mode` flag:
    ```
    python3 ingest_tarball.py my-client-logs-tarball.tar.gz my_client --debug-mode
    ```

    2) To pass in arbitrary config for the filebeat.yml file, use the `--custom-config` flag  send in a key (can be nested) and a value, e.g., 
    ```
    --custom-config setup.kibana.host 123.456.345.123:5601
    ```

    3) To cleanup all generated files if the script run successfully, pass in:
    ```
    --cleanup-on-finish
    ```

    4) ignore zeros in tarball (for when using a combined tarball; see [here](https://www.gnu.org/software/tar/manual/html_node/Ignore-Zeros.html) for what we do)
    ```
    --ignore-zeros 
    ```
    NOTE currently only works with gzipped tarballs (ie file extension tar.gz)

## What does the script do?
  - unzip the tarball
  - Put the logs in the folder we want it in
  - Generate a filebeat.yml for this (will be v0.2; v0.1 just write this ourselves)
  - start filebeat for one-off batch job that ingests these files into ELK 
      * Perhaps later we will just have filebeat running continually on our server, watching  whatever gets placed in

## Want to add some logs and run script again with the same config?
1) Add log files to the directory where similar logs are located: 
  `{self.base_filepath_for_logs}/<hostname>/<type>`.

  e.g., `{self.base_filepath_for_logs}/{example_hostname}/spark/worker/worker.log` 

2) Run filebeat again:

Replace the client_name and incident_id below and run it again
```
sudo filebeat -e -d "*" --c cassandra.vision/cassandra-analyzer/offline-log-ingester/logs-for-client/{client_name}/incident-{incident_id}/tmp/filebeat.yaml
```

- filebeat.yaml will be at: cassandra.vision/cassandra-analyzer/offline-log-ingester/logs-for-client/{client_name}/incident-{incident_id}/tmp/filebeat.yml
- Alternatively, if filebeat is still running (and is using the filebeat.yaml created by this script), you can just add the separate log files and it will find and ingest them. 

## Expected Tarball Format
A number of errors can occur if the tarball or zip file with the logs is not formatted exactly like tarballs received from opscenter or our [`offline-log-collector`](https://github.com/Anant/cassandra.vision/tree/master/cassandra-analyzer/offline-log-collector) tool.

### What we're expecting:
```
<tarball-filename>.tar.gz OR <tarball-filename>.zip
    <archived-dir>/ 
        nodes/
            <ip-1>/
                logs/
                    cassandra/
                        audit/audit.log (optional)
                        system.log
                        gremlin.log (optional)
                        debug.log (optional)
                        output.log (optional)
                        gc.log (optional)
                    spark/
                        master/
                            master.log
                        worker/
                            worker.log
            <ip-2>/
                … (same as ip-1)
            … (other ips)
```

Notes:
- Additional files won't hurt anything. 
- `<archived-dir>` (see above) is often the same name as `<tarball-filename>` but doesn't need to be.
- the absence of optional files mentioned above won't stop the script from running to the end and ingesting whatever files are there, but the presence of those files means those files will be ingested too. See `./helper_classes/filebeat_yml.py` for what files the filebeat ingester is looking for. 
- all log files can either end in `.log` as above, or `.log*`. Often this looks something like `gc.log.0` or `gc.log.3.current`. All that end in `.log*` will be ingested. 

### Example error messages 
#### FileNotFoundError: ...nodes/nodes
```
FileNotFoundError: [Errno 2] No such file or directory: '.../cassandra.vision/cassandra-analyzer/offline-log-ingester/logs-for-client/<client-name>/incident-<incident-id>/tmp/nodes/nodes'
```
This means that the ingested tarball looked something like this: 
```
<tarball-filename>.tar.gz OR <tarball-filename>.zip
        nodes/
            <ip-1>/
            ...
```
The tarball is missing the `<archived-dir>/`, so our script thinks that `nodes` is the name of the `<archived-dir>/`. Consequently, it's looking for `nodes/nodes/` but can't find it. 

To solve, put the `nodes` dir inside a parent directory, rezip your tarball and try again.



### Solutions
- reformat your tarball and zip it back up, and run script again
- Do dangerous stuff if you are feeling confident that you know what you're doing, like going into the `./logs-for-client/` dir where the unzipped logs get put into, and rearrange things there, and temporarily commenting out steps in the `run` method of the `ingest_tarball.py` script, particularly `self.extract_tarball()`, then running `ingest_tarball.py` again. Obviously less than ideal.

## Debugging
### Debugging the filebeat generator
  - Try editing the filebeat.yml manually and running again 
      See [instructions here](#want-to-add-some-logs-and-run-script-again-with-the-same-config) for running again and for where the generated filebeat.yaml is.

#### ERROR: failed to open store 'filebeat': open /var/lib/filebeat/registry/filebeat/meta.json: no such file or directory
This seems to be because newer versions of ES/filebeat (e.g., 7.9.x) have different behavior, and to clear out the registry you actually need to remove the whole `/var/lib/filebeat/registry`, not just the subdirectory `/var/lib/filebeat/registry/filebeat`, which worked before. 

If you don't delete the `/var/lib/filebeat/registry` directory, filebeat doesn't seem to know that it needs to regenerate that directory for you, and throws that error.

Solution: `rm -rf /var/lib/filebeat/registry`

For reference, see [here](https://discuss.elastic.co/t/cant-start-filebeat/181050/7).



### Debugging ES
#### Try sudo filebeat setup
Sometimes filebeat will process logs correctly (which you will be able to see in the filebeat log output, since it will show a log (level DEBUG) for event "Publish event"that shows all the fields. However, it won't get into kibana correctly. Sometimes all it takes is running `sudo filebeat setup` so that filebeat configures for the current elasticsearch setup

Note that by default, `sudo filebeat setup` will use your default filebeat.yml file, which is found at `/etc/filebeat/filebeat.yml`. Make sure those settings are correct, since even running filebeat with a different filebeat.yml will not override some of these configs (especially configs under the `setup` property, e.g., `setup.template.settings`). Those settings only get set when running `filebeat setup`. 

If you want to setup filebeat using a different filebeat.yml file, you can use the `--c` flag, e.g.,:

```
sudo filebeat setup --c cassandra.vision/cassandra-analyzer/offline-log-ingester/logs-for-client/{client_name}/incident-{incident_id}/tmp/filebeat.yaml
```

#### Error: ConnectionError(('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))
Your elasticsearch or kibana hosts might need to be set if you get an error that looks like the following after running the script:
```
elasticsearch.exceptions.ConnectionError: ConnectionError(('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))) caused by: ProtocolError(('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))
```

You can find instructions for doing that above under [Specifying Kibana endpoint](#Specifying Kibana endpoint).

ES host can be set using --es-hosts flag as well.

#### Error: 
```
Elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x7fab672f0780>: Failed to establish a new connection: [Errno 111] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7fab672f0780>: Failed to establish a new connection: [Errno 111] Connection refused)
```

Diagnosing this one:
- There's probably also something like this in the stacktrace:
```
...
File "ingest_tarball.py", line 275, in clear_filebeat_indices_and_registry
self.es.indices.delete(index='filebeat-*')
...
```

- Also you are probably using `--clean-out-filebeat-first` flag, since this is the only thing we're using the python client for right now.

Solution: 
ES Python client isn't connecting to elasticsearch. You probably need to set the --es-hosts flag to something else.


## Testing
### Current way to test: 
Run collect_logs test first. Then run ingest_tarball.py on that test tarball:

```
# if you haven't ran log collection test already...
cd cassandra-analyzer/offline-log-collector
pip3 install -r requirements.txt
pip3 install -r test/requirements.txt
cd test
python3 collect_logs_test.py

# now you have a test tarball to ingest in `offline-log-ingester/log-tarballs-to-ingest`
cd ../../offline-log-ingester
python3 ingest_tarball.py  test_client.tar.gz test_client 
# Or, if you are safe to wipe out filebeat registry and filebeat indices before running the test:
# python3 ingest_tarball.py  test_client.tar.gz test_client --clean-out-filebeat-first

```

### OLD WAY: use the test class
NOTE Currently out of date. 
Would do:
```
  pip3 install -r requirements.txt
  pip3 install -r test/requirements.txt
  cd test
  python3 ingest_tarball_test.py
```

Will need to update `ingest_tarball_test.py` for this to work though.


## Development
### Adding more logs to our tarball
If you want to add more logs from the Cassandra node into the tarball for ingestion:

1) Add another command to `NodeAnalyzer/nodetool.receive.v2.sh` 
  - `collect_logs.py` calls `NodeAnalyzer/nodetool.receive.v2.sh` on each node to get logs and conf files and nodetool output. So to add more files to that list, edit `NodeAnalyzer/nodetool.receive.v2.sh`.
  - Make sure to make a directory for it too e.g., something like:
      ```
      mkdir -p $data_dest_path/<your new path>
      ```

2) If `nodetool.receive.v2.sh` doesn't place the files into a directory that already gets copied, you will have to edit `helper_classes/node.py`
  - `collect_logs.py` will call `helper_classes/node.py` when it is creating the tarball.
  - See `helper_classes/node.py#copy_files_to_final_destination`, which copies all the files for a given node and creates directories in the destination directory if necessary.
  - The files you want copied need to be copied in the `node.py#copy_files_to_final_destination` method, or they will not end up in the tarball at the end.

3) Edit `ingest_tarball.py` to ingest these new files that you want added into Kibana
  - If these are log files that you are adding, Kibana won't see them unless you configure our ingestion tool to do so.
  - `ingest_tarball.py` actually looks at `helper_classes/filebeat_yml.py#log_type_definitions` for what will end up in your filebeat.yml, as well as for what to ingest into kibana. Add a new item in that list in order to ingest your new logs.
      * key (e.g., "spark.master") can be anything as long as it's unique, it is more of a label for us really.
      * `path_to_logs_source` is where the log collection needs to put these logs (corresponds to what you set in `node.py#copy_files_to_final_destination`). These do not need to be unique: e.g., `cassandra.dse-collectd` and `cassandra.garbage_collection` have the same `path_to_logs_source`, and it's no problem. It just means our script will try to copy all these logs twice, which doesn't hurt anything, but it will have two separate entries in our generated filebeat.yml with different paths and different tags, which is what we need.
      * `path_to_logs_dest` is where the log collection will end up after unarchiving and positioning the logs. These do not need to be unique either.
      * `tags` is for separating these logs from other logs, so they are searchable in Kibana. 
      * `log_regex` is the regex that filebeat.yml will use to find htese logs after they are placed by the ingest_tarball.py script. Will include the `path_to_logs_dest` but the regex should include all files you are copying in and exclude files you don't want filebeat to ingest. Files that match will be assigned the `tags` in Kibana. Should be unique as well.
      * if any of the defaults (see 'filebeat_input_template') need to be overwritten, add a key "custom_overwrites" (see `linux.system` logs for example, which uses this).

4) If these are logs that have a pattern different from the other logs that we are ingesting into kibana, you will have to add the pattern into our `config-templates/filebeat.template.yml` file, under the field `processors`.
  - This file contains all dissect patterns.
  - You will probably want to add at least two patterns: 1. for the log pattern itself; 2. One for field: "log.file.path" so that these new logs' filepath gets into kibana correctly also
